# .env file
# Docker image configuration
IMAGE_TAG=main # Docker image tag to use for building the Docker image
PORT=3000 # Port to use for running the web interface

# Database configuration
POSTGRES_PORT=5432 # Port to use for PostgreSQL database
POSTGRES_USER=root # Username for PostgreSQL database
POSTGRES_PASSWORD=mysecretpassword # Password for PostgreSQL database
POSTGRES_HOST=db # Hostname for PostgreSQL database
POSTGRES_DB=local # Database name

# Made using the variables from above. Will be removed in later versions.
# DATABASE_URL=postgres://root:mysecretpassword@db:5432/local # Database URL for connection to PostgreSQL database with credentials from above

# Application configuration
ADMIN_USERNAME=admin # Username for admin user in web interface
ADMIN_PASSWORD=password # Password for admin user in web interface

# AI configuration
# Default Model to use for transcription, can be set to any OpenAI model or Ollama model
# For ollama connections, enter the model name and version number. EG: llama3.2:latest
AI_MODEL="gpt-3.5-turbo"
# Leave blank to use default (OpenAI API), otherwise set to the base URL of your OpenAI API compatible server
# For ollama connections, enter the IP of the Ollama server, and then the port it is running on.
# Include the /v1/ or /api/v1/ path if needed (OpenWeb UI uses /api/ and ollama uses /v1/
# Example: http://192.168.1.5:11434 or http://host.docker.internal:11434
# NOTE: host.docker.internal is only available on Windows and MacOS, use the IP address of the host machine on Linux
# NOTE: localhost and 127.0.0.1 will not work, as they refer to the container itself, not the host machine
OLLAMA_BASE_URL=""

# API Keys
# NOTE:
# If using Ollama, you can leave these blank or set to a dummy value
# If using OpenAI, you must set these to your API keys
# If using a custom API compatible server, you must set these to your API keys
OPENAI_API_KEY="" # Needed for retrieving models from OpenAI, for Ollama connections, this can be left blank or set to a dummy value

# Diarization configuration
# Default Model to use for Diarization, can be set to any compatible model that supports diarization
# NOTE: This model will be downloaded automatically if it is not already present in the models directory
# NOTE: You MUST provide a valid HuggingFace API token with access to pyannote/speaker-diarization models
DIARIZATION_MODEL=pyannote/speaker-diarization@3.0
HUGGINGFACE_TOKEN="" # Required for accessing speaker diarization models from HuggingFace

# Paths
# These almost never need to be changed. They are the paths to the directories where the models and audio files are stored
MODELS_DIR=/scriberr/models
WORK_DIR=/scriberr/temp
AUDIO_DIR=/scriberr/uploads

# Server configuration
BODY_SIZE_LIMIT=1G
HARDWARE_ACCEL=cpu # Set to 'gpu' if you have a Nvidia GPU
USE_WORKER=true # Enable background processing of transcription jobs